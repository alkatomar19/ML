{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# House Price Prediction with Linear Regression\n",
    "\n",
    "In this project, i am going to predict the price of a house using information like its location, area, no. of rooms etc. I'll use the dataset from the [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition on [Kaggle](https://kaggle.com). I'll follow a step-by-step process to train my model:\n",
    "\n",
    "1. Download and explore the data\n",
    "2. Prepare the dataset for training\n",
    "3. Train a linear regression model\n",
    "4. Make predictions and evaluate the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1 - Download and Explore the Data\r\n",
    "\r\n",
    "The dataset is available as a ZIP file at the following url:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_url = 'https://github.com/alkatomar19/MachineLearning/tree/main/HousingPrediction/dataset/house-prices-advanced-regression-techniques.zip'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from urllib.request import urlretrieve"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "urlretrieve(dataset_url, 'house-prices.zip')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from zipfile import ZipFile"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with ZipFile('house-prices.zip') as f:\r\n",
    "    f.extractall(path='house-prices')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is extracted to the folder `house-prices`. Let's view the contents of the folder using the [`os`](https://docs.python.org/3/library/os.html) module."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_dir = 'house-prices'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.listdir(data_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll use the data in the file `train.csv` for training our model. We can load the for processing using the [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html) library."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "pd.options.display.max_columns = 200\r\n",
    "pd.options.display.max_rows = 200"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_csv_path = data_dir + '/train.csv'\r\n",
    "train_csv_path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load the data from the file `train.csv` into a Pandas data frame.\r\n",
    "prices_df = pd.read_csv(train_csv_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#columns and data types within the dataset.\r\n",
    "prices_df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#How many rows and columns does the dataset contain? \r\n",
    "n_rows = prices_df.shape[0]\r\n",
    "n_cols = prices_df.shape[1]\r\n",
    "print('The dataset contains {} rows and {} columns.'.format(n_rows, n_cols))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exploration and visualization of data from the various columns within the dataset, and studying their relationship with the price of the house (using scatter plot and correlations)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import plotly.express as px\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "#The following settings will improve the default style and font sizes for our charts.\r\n",
    "\r\n",
    "sns.set_style('darkgrid')\r\n",
    "matplotlib.rcParams['font.size'] = 14\r\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\r\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = px.scatter(prices_df, \r\n",
    "                 x='TotalBsmtSF', \r\n",
    "                 y='SalePrice', \r\n",
    "                 title='Basement Size vs. Sales Price')\r\n",
    "fig.update_traces(marker_size=5)\r\n",
    "fig.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2 - Prepare the Dataset for Training\n",
    "\n",
    "Before we can train the model, we need to prepare the dataset. Here are the steps we'll follow:\n",
    "\n",
    "1. Identify the input and target column(s) for training the model.\n",
    "2. Identify numeric and categorical input columns.\n",
    "3. [Impute](https://scikit-learn.org/stable/modules/impute.html) (fill) missing values in numeric columns\n",
    "4. [Scale](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) values in numeric columns to a $(0,1)$ range.\n",
    "5. [Encode](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) categorical data into one-hot vectors.\n",
    "6. Split the dataset into training and validation sets.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identify Inputs and Targets\n",
    "\n",
    "While the dataset contains 81 columns, not all of them are useful for modeling. \n",
    "\n",
    "- The first column `Id` is a unique ID for each house and isn't useful for training the model.\n",
    "- The last column `SalePrice` contains the value we need to predict i.e. it's the target column.\n",
    "- Data from all the other columns (except the first and the last column) can be used as inputs to the model.\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prices_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Identify the input columns (a list of column names)\r\n",
    "input_cols = prices_df.columns.difference(['Id','SalePrice'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Identify the name of the target column (a single string, not a list)\r\n",
    "target_col = 'SalePrice'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure that the `Id` and `SalePrice` columns are not included in `input_cols`.\n",
    "\n",
    "Now that we've identified the input and target columns, we can separate input & target data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs_df = prices_df[input_cols].copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "targets = prices_df[target_col]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "inputs_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Identify Numeric and Categorical Data\n",
    "\n",
    "The next step in data preparation is to identify numeric and categorical columns. We can do this by looking at the data type of each column."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prices_df.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will Create two lists `numeric_cols` and `categorical_cols` containing names of numeric and categorical input columns within the dataframe respectively. Numeric columns have data types `int64` and `float64`, whereas categorical columns have the data type `object`. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "numeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\r\n",
    "categorical_cols = inputs_df.select_dtypes(include=['object']).columns.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(list(numeric_cols))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(list(categorical_cols))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\r\n",
    "missing_counts[missing_counts > 0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I'll replace missing values with the average value in the column using the `SimpleImputer` class from `sklearn.impute`.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.impute import SimpleImputer\r\n",
    "# 1. Create the imputer\r\n",
    "imputer = SimpleImputer(strategy='mean')\r\n",
    "# 2. Fit the imputer to the numeric colums\r\n",
    "imputer.fit(inputs_df[numeric_cols])\r\n",
    "# 3. Transform and replace the numeric columns\r\n",
    "inputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])\r\n",
    "\r\n",
    "#After imputation, none of the numeric columns should contain any missing values.\r\n",
    "missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\r\n",
    "missing_counts[missing_counts > 0] # should be an empty list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Scale Numerical Values\r\n",
    "\r\n",
    "#The numeric columns in our dataset have varying ranges. \r\n",
    "inputs_df[numeric_cols].describe().loc[['min', 'max']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "# Create the scaler\r\n",
    "scaler = MinMaxScaler()\r\n",
    "# Fit the scaler to the numeric columns\r\n",
    "scaler.fit(inputs_df[numeric_cols])\r\n",
    "# Transform and replace the numeric columns\r\n",
    "inputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#After scaling, the ranges of all numeric columns should be $(0, 1)$.\r\n",
    "inputs_df[numeric_cols].describe().loc[['min', 'max']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Encode Categorical Columns\r\n",
    "\r\n",
    "#Our dataset contains several categorical columns, each with a different number of categories.\r\n",
    "inputs_df[categorical_cols].nunique().sort_values(ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "# 1. Create the encoder\r\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\r\n",
    "# 2. Fit the encoder to the categorical colums\r\n",
    "encoder.fit(inputs_df[categorical_cols])\r\n",
    "# 3. Generate column names for each category\r\n",
    "encoded_cols = list(encoder.get_feature_names(categorical_cols))\r\n",
    "len(encoded_cols)\r\n",
    "# 4. Transform and add new one-hot category columns\r\n",
    "inputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and Validation Set\n",
    "\n",
    "Finally, let's split the dataset into a training and validation set. I'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs_df[numeric_cols + encoded_cols], targets, test_size=0.25, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3 - Train a Linear Regression Model\n",
    "\n",
    "We're now ready to train the model. I'll use Ridge Regression, a variant of linear regression that uses a technique called L2 regularization to introduce another loss term that forces the model to generalize better. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import Ridge\r\n",
    "# Create the model\r\n",
    "model = Ridge(random_state=42)\r\n",
    "# Fit the model using inputs and targets\r\n",
    "model.fit(train_inputs[numeric_cols + encoded_cols], train_targets)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4 - Make Predictions and Evaluate Your Model\n",
    "\n",
    "The model is now trained, and we can use it to generate predictions for the training and validation inputs. We can evaluate the model's performance using the RMSE (root mean squared error) loss function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\r\n",
    "X_train = train_inputs[numeric_cols + encoded_cols]\r\n",
    "X_val = val_inputs[numeric_cols + encoded_cols]\r\n",
    "train_preds = model.predict(X_train)\r\n",
    "train_rmse = mean_squared_error(train_targets, train_preds,squared=False)\r\n",
    "print('The RMSE loss for the training set is $ {}.'.format(train_rmse))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "val_preds = model.predict(X_val)\r\n",
    "val_rmse =mean_squared_error(val_targets,val_preds,squared=False)\r\n",
    "print('The RMSE loss for the validation set is $ {}.'.format(val_rmse))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Importance\n",
    "\n",
    "Let's look at the weights assigned to different columns, to figure out which columns in the dataset are the most important."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weights = model.coef_\r\n",
    "weights_df = pd.DataFrame({\r\n",
    "    'columns': train_inputs.columns,\r\n",
    "    'weight': weights\r\n",
    "}).sort_values('weight', ascending=False)\r\n",
    "weights_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Making Predictions\n",
    "\n",
    "The model can be used to make predictions on new inputs using the following helper function:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def predict_input(single_input):\r\n",
    "    input_df = pd.DataFrame([single_input])\r\n",
    "    input_df[numeric_cols] = imputer.transform(input_df[numeric_cols])\r\n",
    "    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])\r\n",
    "    input_df[encoded_cols] = encoder.transform(input_df[categorical_cols].values)\r\n",
    "    X_input = input_df[numeric_cols + encoded_cols]\r\n",
    "    return model.predict(X_input)[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_input = { 'MSSubClass': 20, 'MSZoning': 'RL', 'LotFrontage': 77.0, 'LotArea': 9320,\r\n",
    " 'Street': 'Pave', 'Alley': None, 'LotShape': 'IR1', 'LandContour': 'Lvl', 'Utilities': 'AllPub',\r\n",
    " 'LotConfig': 'Inside', 'LandSlope': 'Gtl', 'Neighborhood': 'NAmes', 'Condition1': 'Norm', 'Condition2': 'Norm',\r\n",
    " 'BldgType': '1Fam', 'HouseStyle': '1Story', 'OverallQual': 4, 'OverallCond': 5, 'YearBuilt': 1959,\r\n",
    " 'YearRemodAdd': 1959, 'RoofStyle': 'Gable', 'RoofMatl': 'CompShg', 'Exterior1st': 'Plywood',\r\n",
    " 'Exterior2nd': 'Plywood', 'MasVnrType': 'None','MasVnrArea': 0.0,'ExterQual': 'TA','ExterCond': 'TA',\r\n",
    " 'Foundation': 'CBlock','BsmtQual': 'TA','BsmtCond': 'TA','BsmtExposure': 'No','BsmtFinType1': 'ALQ',\r\n",
    " 'BsmtFinSF1': 569,'BsmtFinType2': 'Unf','BsmtFinSF2': 0,'BsmtUnfSF': 381,\r\n",
    " 'TotalBsmtSF': 950,'Heating': 'GasA','HeatingQC': 'Fa','CentralAir': 'Y','Electrical': 'SBrkr', '1stFlrSF': 1225,\r\n",
    " '2ndFlrSF': 0, 'LowQualFinSF': 0, 'GrLivArea': 1225, 'BsmtFullBath': 1, 'BsmtHalfBath': 0, 'FullBath': 1,\r\n",
    " 'HalfBath': 1, 'BedroomAbvGr': 3, 'KitchenAbvGr': 1,'KitchenQual': 'TA','TotRmsAbvGrd': 6,'Functional': 'Typ',\r\n",
    " 'Fireplaces': 0,'FireplaceQu': np.nan,'GarageType': np.nan,'GarageYrBlt': np.nan,'GarageFinish': np.nan,'GarageCars': 0,\r\n",
    " 'GarageArea': 0,'GarageQual': np.nan,'GarageCond': np.nan,'PavedDrive': 'Y', 'WoodDeckSF': 352, 'OpenPorchSF': 0,\r\n",
    " 'EnclosedPorch': 0,'3SsnPorch': 0, 'ScreenPorch': 0, 'PoolArea': 0, 'PoolQC': np.nan, 'Fence': np.nan, 'MiscFeature': 'Shed',\r\n",
    " 'MiscVal': 400, 'MoSold': 1, 'YrSold': 2010, 'SaleType': 'WD', 'SaleCondition': 'Normal'}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predicted_price = predict_input(sample_input)\r\n",
    "print('The predicted sale price of the house is ${}'.format(predicted_price))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the model\n",
    "\n",
    "Let's save the model (along with other useful objects) to disk, so that we use it for making predictions without retraining."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import joblib\r\n",
    "house_price_predictor = {\r\n",
    "    'model': model,\r\n",
    "    'imputer': imputer,\r\n",
    "    'scaler': scaler,\r\n",
    "    'encoder': encoder,\r\n",
    "    'input_cols': input_cols,\r\n",
    "    'target_col': target_col,\r\n",
    "    'numeric_cols': numeric_cols,\r\n",
    "    'categorical_cols': categorical_cols,\r\n",
    "    'encoded_cols': encoded_cols\r\n",
    "}\r\n",
    "joblib.dump(house_price_predictor, 'house_price_predictor.joblib')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}